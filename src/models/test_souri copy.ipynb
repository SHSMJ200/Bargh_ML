{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5c6ae3f",
   "metadata": {},
   "source": [
    "IMPORT LIBRARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "756b96513e0197a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-11T16:51:32.719853Z",
     "start_time": "2025-08-11T16:49:28.597647200Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e:\\univercity\\project\\Bargh_ML\\logs\\model_main.log\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "current_dir = os.getcwd()\n",
    "project_root = current_dir[:current_dir.find(\"src\") - 1]\n",
    "sys.path.insert(0, project_root)\n",
    "\n",
    "import pandas as pd\n",
    "from data_selector import Data_selector\n",
    "from feature_modifier import Feature_selector, Feature_adder\n",
    "from logs.logger import CustomLogger\n",
    "from models import Random_Forest, Linear\n",
    "from src.root import get_root\n",
    "\n",
    "logger = CustomLogger(name=\"model_main\", log_file_name='model_main.log').get_logger()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5ec5a3",
   "metadata": {},
   "source": [
    "READ CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88d09945",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_time(df,date1,date2,get_bool=False):\n",
    "    if get_bool:\n",
    "        return (df['datetime'] >= date1) & (df['datetime'] <= date2)\n",
    "    return df[(df['datetime'] >= date1) & (df['datetime'] <= date2)]\n",
    "\n",
    "def filter_name_code(df,name,code,get_bool=False):\n",
    "    if get_bool:\n",
    "        return (df[\"name\"] == name) & (df[\"code\"] == code)\n",
    "    return df[(df[\"name\"] == name) & (df[\"code\"] == code)]\n",
    "\n",
    "def get_interval(df,l_min):\n",
    "    df_s = df.reset_index(drop=True)\n",
    "    diff = df_s['datetime'].diff()  # محاسبه اختلاف زمانی بین ردیف ها\n",
    "    gap_mask = diff != pd.Timedelta(hours=1)  # هر جایی اختلاف دقیقاً 1 ساعت نیست، مرز بازه جدید است\n",
    "    # ایندکس شروع بازه‌ها\n",
    "    start_indices = df_s.index[gap_mask].tolist()\n",
    "    # چون اولین ایندکس هم ابتدای یک بازه است، اگر نیست اضافه می‌کنیم\n",
    "    if 0 not in start_indices:\n",
    "        start_indices = [0] + start_indices\n",
    "    # ایندکس پایان بازه‌ها یکی قبل از شروع بازه بعدی است\n",
    "    end_indices = [i-1 for i in start_indices[1:]] + [df_s.index[-1]]\n",
    "    \n",
    "    for i in range(len(start_indices)-1,-1,-1):\n",
    "        if end_indices[i] - start_indices[i] < l_min-1:\n",
    "            end_indices.pop(i)\n",
    "            start_indices.pop(i)\n",
    "    # ساخت لیست بازه‌های (i1, i2)\n",
    "    index_ranges = list(zip(start_indices, end_indices))\n",
    "    \n",
    "    # ساخت لیست بازه‌های زمانی (t1, t2)\n",
    "    time_ranges = [(df_s.loc[i1, 'datetime'], df_s.loc[i2, 'datetime']) for i1, i2 in index_ranges]\n",
    "    return index_ranges,time_ranges\n",
    "\n",
    "import numpy as np\n",
    "from scipy.signal import savgol_filter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def is_smooth_array(data,threshold,f_show = False):\n",
    "    x = data[\"datetime\"].to_numpy()  # Your x-values\n",
    "    y = data[\"generation\"].to_numpy()  # Your y-values\n",
    "    window_length = len(y) - 1 + len(y)%2  # window size, must be odd, adjust for your data\n",
    "    \n",
    "    polyorder = 2  # polynomial order\n",
    "    y_smooth = savgol_filter(y, window_length=window_length, polyorder=polyorder)\n",
    "\n",
    "    # Compute residual noise\n",
    "    residuals = y - y_smooth\n",
    "    noise_std = np.std(residuals)\n",
    "    value = np.var(np.diff(y, n=1))#abs(y-y.mean())#residuals\n",
    "    \n",
    "    # Optional: Visualize\n",
    "    if f_show:\n",
    "        plt.plot(x, y, label='Raw data')\n",
    "        plt.plot(x, y_smooth, label='Smoothed curve')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    if noise_std < threshold:  # threshold is your chosen value for smoothness\n",
    "        return noise_std,True,value\n",
    "    else:\n",
    "        return noise_std,False,value\n",
    "\n",
    "def is_smooth(df,date1,date2,data_var,data_date,threshold,flag=False):\n",
    "    slice = filter_time(df,date1,date2)[[\"hour\",\"generation\",\"datetime\"]]\n",
    "    if len(slice[\"hour\"]) >= 8:\n",
    "        x,f,re = is_smooth_array(slice,threshold,flag)\n",
    "        if data_var != None : data_var[f].append(x)\n",
    "        if data_date != None : data_date[f].append((date1,date2))\n",
    "        if f:    \n",
    "            return 1,1\n",
    "        return 1,0\n",
    "    return 0,0\n",
    "\n",
    "def get_smooth_good_slice(df,time_ranges,threshold):\n",
    "    data_var = {True:[],False:[]}\n",
    "    data_date = {True:[],False:[]}\n",
    "    t = 0\n",
    "    n = 0\n",
    "    for date1,date2 in time_ranges:\n",
    "        nn,tt = is_smooth(df,date1,date2,data_var,data_date,threshold)\n",
    "        n += nn\n",
    "        t += tt\n",
    "    p = t/n*100 if n != 0 else None\n",
    "    return p,data_var,data_date\n",
    "\n",
    "def labeling_point(df,df_n_c,date,label):\n",
    "    for date1,date2 in date:\n",
    "        flag_array = filter_time(df_n_c,date1,date2,get_bool=True)\n",
    "        df.loc[flag_array.index[flag_array],\"is_good_pick\"] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8170e3ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "پرند G16 37.714285714285715\n",
      "شهدای پیروز - بهبهان G12 65.3179190751445\n",
      "شهدای پیروز - بهبهان S1 46.85714285714286\n",
      "جنوب اصفهان - چهلستون G15 41.95402298850575\n",
      "سیکل ترکیبی شیروان S1 12.578616352201259\n",
      "سیکل ترکیبی شیروان G11 8.405797101449275\n",
      "عسلویه G14 42.35294117647059\n",
      "عسلویه G15 21.965317919075144\n",
      "سیکل ترکیبی ارومیه G13 51.445086705202314\n",
      "سیکل ترکیبی ارومیه G14 52.87356321839081\n",
      "عسلویه G16 50.29239766081871\n",
      "پرند G15 38.372093023255815\n",
      "پرند G14 35.26011560693642\n",
      "شهدای پیروز - بهبهان G11 76.3586956521739\n",
      "جنوب اصفهان - چهلستون G11 54.76839237057221\n",
      "سبلان G11 54.78260869565217\n",
      "سیکل ترکیبی ارومیه G11 50.71633237822349\n",
      "سیکل ترکیبی ارومیه G16 25.581395348837212\n",
      "سیکل ترکیبی شیروان S2 14.545454545454545\n",
      "سیکل ترکیبی شیروان G12 7.926829268292683\n",
      "سیکل ترکیبی شیروان G13 8.125\n",
      "عسلویه G11 62.15469613259669\n",
      "حافظ G11 46.72131147540984\n",
      "جنوب اصفهان - چهلستون G14 45.40229885057471\n",
      "جنوب اصفهان - چهلستون G16 44.57142857142857\n",
      "سیکل ترکیبی ارومیه G15 24.418604651162788\n",
      "سیکل ترکیبی ارومیه G12 59.30232558139535\n",
      "سیکل ترکیبی شیروان G16 8.333333333333332\n",
      "حافظ G16 40.0\n",
      "پرند G13 38.72832369942196\n",
      "سبلان G12 65.11627906976744\n",
      "سیکل ترکیبی یزد G12 99.42196531791907\n",
      "سبلان G16 23.837209302325583\n",
      "پرند G11 56.403269754768395\n",
      "پرند G12 58.857142857142854\n",
      "جنوب اصفهان - چهلستون G12 45.14285714285714\n",
      "جنوب اصفهان - چهلستون G13 44.57142857142857\n",
      "سبلان G15 25.0\n",
      "سیکل ترکیبی شیروان G14 15.950920245398773\n",
      "عسلویه G12 54.59770114942529\n",
      "حافظ G13 39.08045977011494\n",
      "عسلویه G13 45.22292993630573\n",
      "حافظ G12 38.285714285714285\n",
      "سبلان G13 25.190839694656486\n",
      "سبلان G14 31.3953488372093\n",
      "حافظ G15 47.701149425287355\n",
      "سیکل ترکیبی شیروان G15 8.974358974358974\n",
      "حافظ G14 41.37931034482759\n",
      "سیکل ترکیبی یزد G11 99.17582417582418\n",
      "سبلان S1 57.3170731707317\n",
      "عسلویه S1 14.814814814814813\n"
     ]
    }
   ],
   "source": [
    "df_row = pd.read_csv(get_root() +\"/data/processed/integrated.csv\", encoding='utf-8')\n",
    "df_modified = df_row[[\"name\",\"code\",\"date\",\"hour\",\"status\",'value',\"generation\"]].copy(deep=True)\n",
    "\n",
    "feature_adder = Feature_adder(df_modified)\n",
    "feature_adder.add_season()\n",
    "df_modified = Data_selector(df_modified).select(m_in_summer=True)\n",
    "\n",
    "threshold = 2\n",
    "\n",
    "df_modified['datetime'] = df_modified['date'] + pd.to_timedelta(df_row['hour'], unit='h')\n",
    "df_row[\"is_good_pick\"] = 0\n",
    "\n",
    "power_plants = df_modified[['name', 'code']].drop_duplicates()\n",
    "n = len(power_plants)\n",
    "\n",
    "for _, row in power_plants.iterrows():\n",
    "    \n",
    "    df_name_code_smooth = filter_name_code(df_modified,row[\"name\"],row[\"code\"])\n",
    "    \n",
    "    index_ranges,time_ranges = get_interval(df_name_code_smooth,l_min=4)\n",
    "    #print(k,\"->\",np.unique(np.array([(t2-t1).total_seconds()/3600+1 for t1,t2 in time_ranges]),return_counts=True))\n",
    "    p,data_var,data_date = get_smooth_good_slice(df=df_name_code_smooth,time_ranges=time_ranges,threshold=threshold)\n",
    "    if p != 0 and p != None:\n",
    "        #print(\"**********\")\n",
    "        print(row[\"name\"],row[\"code\"],p)\n",
    "        #print(len(data_date[True]),len(data_date[False]))\n",
    "        #print(k,\"->\",np.unique(np.array([(t2-t1).total_seconds()/3600+1 for t1,t2 in time_ranges]),return_counts=True))\n",
    "    \n",
    "    labeling_point(df_row,df_name_code_smooth,data_date[False],label=1)\n",
    "    labeling_point(df_row,df_name_code_smooth,data_date[True],label=2)\n",
    "    \n",
    "df_row.to_csv(path_or_buf=get_root() +'/data/processed/prediction_only.csv', sep=',', header=True, index=False,na_rep='NULL')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2fa6ffd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "339385"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.count_nonzero(df_row[\"is_good_pick\"].to_numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d22a929",
   "metadata": {},
   "source": [
    "# + MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e31805c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "current_dir = os.getcwd()\n",
    "project_root = current_dir[:current_dir.find(\"src\") - 1]\n",
    "sys.path.insert(0, project_root)\n",
    "\n",
    "import pandas as pd\n",
    "from data_selector import Data_selector\n",
    "from feature_modifier import Feature_selector, Feature_adder\n",
    "from logs.logger import CustomLogger\n",
    "from src.models.models import Random_Forest, Linear, Polynomial, XGBoost\n",
    "\n",
    "logger = CustomLogger(name=\"model_main\", log_file_name='model_main.log').get_logger()\n",
    "\n",
    "csv_path = os.path.join(project_root, \"data\", \"processed\", \"prediction_only.csv\")\n",
    "df = pd.read_csv(csv_path, encoding='utf-8')\n",
    "logger.info(f\"Csv file has bean read successfully\")\n",
    "\n",
    "feature_adder = Feature_adder(df)\n",
    "feature_adder.add_season()\n",
    "# feature_adder.create_feature_with_delay(\"temperature\", 1)\n",
    "# feature_adder.create_feature_with_delay(\"temperature\", 2)\n",
    "# feature_adder.create_feature_with_delay(\"temperature\", 3)\n",
    "logger.info(f\"Some features have been added successfully\")\n",
    "\n",
    "logger.info(f\"Rows have been selected successfully\")\n",
    "df_modified = df.copy()\n",
    "df_modified = Data_selector(df_modified).select(m_in_summer=True)\n",
    "feature_selector = Feature_selector(df_modified, \"generation\")\n",
    "feature_to_be_dropped = ['id', 'date', 'declare', 'require', 'dew', 'apparent_temperature', 'rain', 'snow',\n",
    "                            'evapotransporation', 'wind_direction']\n",
    "\n",
    "\n",
    "X, y = feature_selector.select(feature_to_be_dropped)\n",
    "y = y[X['is_good_pick'] == 2]\n",
    "X = X[X[\"is_good_pick\"] == 2]\n",
    "\n",
    "y = y[((X['status'] == 'SO') | (X['status'] == 'LF1'))]\n",
    "X = X[((X['status'] == 'SO') | (X['status'] == 'LF1'))]\n",
    "\n",
    "peak_condition = (X['value'] == 'P') | (X['value'] == 'M') & (X['season'] == 'summer')\n",
    "y = y[peak_condition]\n",
    "peak_condition = (X['value'] == 'P') | (X['value'] == 'M') & (X['season'] == 'summer')\n",
    "X = X[peak_condition]\n",
    "\n",
    "logger.info(f\"Some features have been dropped successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e79d15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1591896, 459689, 80617)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df),len(df_modified),len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f366878a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "depth 10 Train Error: 3.70% Test Error: 4.04%\n"
     ]
    }
   ],
   "source": [
    "n_est = 40 # 40\n",
    "depth = 10 # 22\n",
    "model = Random_Forest()\n",
    "model.scale_and_split_data(X, y)\n",
    "\n",
    "train_err = []\n",
    "test_err = []\n",
    "#for i in range(1,depth+1): \n",
    "model.fit(n_estimators=n_est, max_depth=depth)\n",
    "mse_train_actual, mse_test_actual = model.compute_mse_error()\n",
    "print(\"depth\",depth,f\"Train Error: {mse_train_actual:0.2f}%\",f\"Test Error: {mse_test_actual:0.2f}%\")\n",
    "train_err.append(mse_train_actual)\n",
    "test_err.append(mse_test_actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad17a58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1d1a4770",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-08-19 05:37:49\u001b[0m - \u001b[34mmodel_main\u001b[0m - \u001b[1;30mINFO\u001b[0m - Csv file has bean read successfully\n"
     ]
    }
   ],
   "source": [
    "csv_path = os.path.join(project_root, \"data\", \"processed\", \"prediction_only.csv\")\n",
    "df = pd.read_csv(csv_path, encoding='utf-8')\n",
    "logger.info(f\"Csv file has bean read successfully\")\n",
    "\n",
    "feature_adder = Feature_adder(df)\n",
    "feature_adder.add_season()\n",
    "\n",
    "df_modified = df.copy(deep=True)\n",
    "feature_selector = Feature_selector(df_modified, \"generation\")\n",
    "feature_to_be_dropped = ['id', 'date', 'declare', 'require', 'dew', 'apparent_temperature', 'rain', 'snow',\n",
    "                            'evapotransporation', 'wind_direction']\n",
    "\n",
    "X, y = feature_selector.select(feature_to_be_dropped)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
